{"name":"Preceptor","tagline":"Test runner and aggregator","body":"![Preceptor Logo](https://raw.githubusercontent.com/yahoo/preceptor/master/images/logo1.png)\r\n\r\nPreceptor is a test-runner and test-aggregator that runs multiple tests and testing frameworks in parallel, sequential, or a combination there of, aggregating all of the test-results and coverage-reports.\r\n\r\n\r\n[![Build Status](https://secure.travis-ci.org/yahoo/preceptor.png)](http://travis-ci.org/yahoo/preceptor)\r\n[![npm version](https://badge.fury.io/js/preceptor.svg)](http://badge.fury.io/js/preceptor)\r\n\r\n[![NPM](https://nodei.co/npm/preceptor.png?downloads=true)](https://nodei.co/npm/preceptor/)\r\n\r\n\r\n[API-Documentation](http://yahoo.github.io/preceptor/docs/)\r\n\r\n[Coverage Report](http://yahoo.github.io/preceptor/coverage/lcov-report/)\r\n\r\n##Quick Look\r\nMain-Features:\r\n* Runs tests sequentially, in parallel, or a combination thereof.\r\n* Support of multiple commonly used testing frameworks including: Mocha, Cucumber, and Kobold.\r\n* Test-report aggregation, combining all of the test-results in one place, supporting a big list of commonly used reporters, including display reporters such as “Spec” and “Dot” and file-format reporters such as “JUnit” and “TAP”, all of which can be used at the same time.\r\n* Centralized collection of code-coverage metrics for every test that ran, merging all of the results in one place, giving the developers and testers a complete picture of what has been tested, and what might needs more tests.\r\n\r\nFeatures through plugins:\r\n* Selenium test management, starting-up and tearing-down Selenium servers transparently for the developers and testers, injecting code to the test-client, reducing with it the boilerplate code needed to run Selenium tests. This will also help simplify the configuration of the Selenium test system.\r\n* Client-side code-coverage collection, merging the code-coverage reports of an application that ran in a browser.\r\n\r\n\r\n\r\n**Table of Contents**\r\n* [Installation](#installation)\r\n* [What is Preceptor?](#what-is-preceptor)\r\n* [Getting Started](#getting-started)\r\n* [Usage](#usage)\r\n    * [Command-Line Usage](#command-line-usage)\r\n    * [Testing Lifecycle](#testing-lifecycle)\r\n        * [Administrative states](#administrative-states)\r\n        * [Suite management](#suite-management)\r\n        * [Test management](#test-management)\r\n    * [Configuration](#configuration)\r\n        * [Global configuration](#global-configuration)\r\n            * [Reporting](#reporting)\r\n            * [Coverage](#coverage)\r\n            * [Plugins](#plugins)\r\n        * [Shared configuration](#shared-configuration)\r\n    * [Tasks](#tasks)\r\n        * [General configuration](#general-configuration)\r\n        * [Cucumber Task](#cucumber-task)\r\n        * [Mocha Task](#mocha-task)\r\n        * [Kobold Task](#kobold-task)\r\n        * [Loader Task](#loader-task)\r\n        * [Node Task](#node-task)\r\n        * [Shell Task](#shell-task)\r\n        * [Group Task](#group-task)\r\n    * [Task Decorators](#task-decorators)\r\n    * [Client Decorators](#client-decorators)\r\n    * [Client Runner](#client-runner)\r\n    * [Plugin Naming](#plugin-naming)\r\n* [API-Documentation](#api-documentation)\r\n* [Tests](#tests)\r\n* [Third-party libraries](#third-party-libraries)\r\n* [License](#license)\r\n\r\n\r\n##Installation\r\n\r\nInstall this module with the following command:\r\n```shell\r\nnpm install preceptor\r\n```\r\n\r\nTo add the module to your ```package.json``` dependencies:\r\n```shell\r\nnpm install --save preceptor\r\n```\r\nTo add the module to your ```package.json``` dev-dependencies:\r\n```shell\r\nnpm install --save-dev preceptor\r\n```\r\n\r\n##What is Preceptor?\r\n\r\nToday, there are a lot of testing frameworks out there, many of which are optimized for testing specific areas. A couple of these frameworks are:\r\n* *Mocha* - The most popular unit-testing tool (according to NPM downloads), testing each individual unit/component of a system in isolation.\r\n* *Cucumber* - A high-level acceptance testing tool that tests features through scenarios, giving product owners and teams a higher degree of collaboration, and management a deeper understanding of the systems and to what extend it is tested.\r\n* *Kobold* - A visual regression testing tool, making sure that any visual changes are recognized before the code goes out.\r\n* ...\r\n\r\nAll of these testing-frameworks produce their own test results - often in different output formats. Additional testing tools, like Selenium, create even more results, generating the results for tests that were run in multiple browsers. CI systems help here a bit since they can usually handle multiple results, but all the results are usually plugged into one root-node, making it hard to organize and analyse the test-results. Some engineers spend a good amount of time aggregating all of these results in a reasonable manner, creating often one-off hacks to merge all the results. Others just manually parse through the results, determining if any kind of problems were encountered during the most recent test-run, loosing the ability to have a one-look green/red summary.\r\n\r\nCode-coverage metrics pose a similar problem: code-coverage reports are created independently and in isolation of tests that run for the same system in different testing frameworks or in separate tests (unit, component, integration, functional, acceptance, ...). It is challenging to have a complete overview of all coverage metrics as there is never only one coverage-report for all the tests. What is usually the case is that only some tests have coverage reports, possibly having only a subset of the metrics that could have been collected - most testing systems, for example, are not collecting client-side coverage-reports when running integration tests with Selenium; these are valuable coverage results.\r\n\r\nIn addition, all of these tools need to be coordinated, running sequentially, in parallel, or even a combination thereof. However, many testing tools do not have this capability build-in, resulting in third-party developers create their own solutions which are quite often not much more than a hack, having also quite possibly inconsistent interfaces as they are usually not created by the same people. All of this coordination can easily turn the code-base into an unmaintainable mess, quite possibly triggering false-positives due to flaky testing infrastructure and code.\r\n\r\nPreceptor tries to solve all of these problems by using a consistent way of configuring the tools, managing the workflow within a single configuration file, and aggregating the test-results and code-coverage reports in one place. In addition, it adds a flexible and extensible plugin infrastructure to the test-runner, introducing even more features like a Selenium / WebDriver integration, injecting client/server code in setup/teardown methods of different testing frameworks, keeping the test-code free of glue-code.\r\n\r\nPreceptor comes with the following features out of the box:\r\n* Test-report aggregation, combining all of the test-results in one place, supporting a big list of commonly used reporters, including display reporters such as “Spec” and “Dot” and file-format reporters such as “JUnit” and “TAP”, all of which can be used at the same time.\r\n* Centralized collection of code-coverage metrics for every test that ran, merging all of the results in one place, giving the developers, testers, and managers a complete picture of what has been tested, and what might need more tests.\r\n* Coordination of tests, queuing them, running them in parallel, or composing them - any way the developer needs or wants them to run.\r\n* Support of multiple commonly used testing frameworks including: Mocha, Cucumber, and Kobold. Support for other frameworks and test-runners like Jasmine and Karma are a WIP.\r\n* Report listeners to parse results from unsupported testing-framework through the usage of commonly used protocols like the TeamCity test-result protocol that is used by the IntelliJ IDEs.\r\n\r\nAs mentioned, plugins add even more features, including:\r\n* Selenium test management, starting-up and tearing-down Selenium servers transparently for the developers and testers, injecting code to the test-client, reducing with it the boilerplate code needed to run Selenium tests. This will also help simplify the configuration of the Selenium test system.\r\n* Client-side code-coverage collection, merging the code-coverage reports of an application that ran in a browser.\r\n\r\n##Getting Started\r\n\r\nPreceptor comes with a command-line tool that takes a configuration file that defines what sequence the task should run, what data needs to be collected, and how some tasks affect others.\r\n\r\nLet's have a look at some examples that uses Mocha as sole testing tool. Preceptor can be used to mix multiple testing frameworks and independent tests at the same time, however, I will only use Mocha for the next examples.\r\n\r\nTo get started, create the following file with the name ```config.js```:\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\" },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false }\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\r\n\t\"tasks\": [\r\n\t\t{ // Task\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Everything\", // Just giving the task a title\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t}\r\n\t\t}\r\n\t]\r\n};\r\n```\r\nThis configuration adds a Mocha task and two reports:\r\n* ```Spec``` - A detailed console report that is very similar to Mocha's ```spec``` reporter.\r\n* ```List``` - Comprehensive list of all problems encountered which will be printed add the end of all test-runs.\r\n\r\nAs SUT, add the following ```lib.js``` file to the same folder:\r\n```javascript\r\nconsole.log('Mostly Harmless');\r\n\r\nmodule.exports = {\r\n\r\n  answerToLifeTheUniverseAndEverything: function () {\r\n    return 42;\r\n  },\r\n\r\n  whatToDo: function (phrase) {\r\n    return (phrase == \"don't panic\");\r\n  },\r\n\r\n  allThereIs: function () {\r\n    return 6 * 9;\r\n  },\r\n\r\n  worstFirstMillionYears: function () {\r\n    return 10 + 10 + 10;\r\n  },\r\n\r\n  message: function () {\r\n    console.log(\"So Long, and Thanks for All the Fish.\");\r\n  },\r\n\r\n  startInfiniteImprobabilityDriver: function () {\r\n    throw new Error(\"infinitely improbable\");\r\n  }\r\n};\r\n```\r\n\r\nThe tests live in a sub-folder with the name ```mocha```. For now, let's add a couple of simple tests that give you a glimpse on how test-results will look.\r\n\r\nCopy the following content into ```mocha/test1.js```:\r\n```javascript\r\nvar assert = require('assert');\r\nvar lib = require('../lib');\r\n\r\nit('should know the answer to life, the universe, and everything', function () {\r\n\tassert.equal(lib.answerToLifeTheUniverseAndEverything(), 42);\r\n});\r\n\r\ndescribe('The End', function () {\r\n\r\n\tit('should print something', function () {\r\n\t\tlib.message();\r\n\t});\r\n});\r\n```\r\n\r\nPreceptor adds a command-line tool that can be executed by calling ```preceptor``` in the shell (when installed globally). Run Preceptor with the above created configuration file:\r\n```shell\r\npreceptor config.js\r\n```\r\nThe result should look something like this (colors are not shown):\r\n```shell\r\n  ✓ should know the answer to life, the universe, and everything\r\n\r\n  The End\r\n    ✓ should print something\r\n```\r\nYou can see that the output that was created in ```lib.js``` isn't visible anywhere. By default, Preceptor will hide any output created by tests, the SUT, and the testing-framework, displaying only the output of loaded reporters. This behavior can be changed by modifying values in the task-options for each individual tasks. For more information on how to do this, please refer to the API documentation or see below.\r\n\r\nNow, to show some of the features, let's add two more test files:\r\n\r\n**mocha/test2.js**\r\n```javascript\r\nvar assert = require('assert');\r\nvar lib = require('../lib');\r\n\r\nit('should find all there is', function () {\r\n\tassert.equal(lib.allThereIs(), 42);\r\n});\r\n\r\nit('should calculate the worst first million years for marvin', function () {\r\n\tassert.equal(lib.worstFirstMillionYears(), 30);\r\n});\r\n```\r\n\r\n**mocha/test3.js**\r\n```javascript\r\nvar assert = require('assert');\r\nvar lib = require('../lib');\r\n\r\nit('should not panic', function () {\r\n\tassert.ok(lib.whatToDo(\"don't panic\"));\r\n});\r\n\r\nit('should panic', function () {\r\n\tassert.ok(!lib.whatToDo(\"do not panic\"));\r\n});\r\n```\r\n\r\nLet's run them all sequentially. Simply add them to the task-list:\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\" },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false }\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\r\n\t\"tasks\": [\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Everything\",\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t}\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Calculations\",\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test2.js\"]\r\n\t\t\t}\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Phrasing\",\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test3.js\"]\r\n\t\t\t}\r\n\t\t}\r\n\t]\r\n};\r\n```\r\nRun Preceptor:\r\n```shell\r\n  ✓ should know the answer to life, the universe, and everything\r\n\r\n  The End\r\n    ✓ should print something\r\n  1) should find all there is\r\n  ✓ should calculate the worst first million years for marvin\r\n  ✓ should not panic\r\n  ✓ should panic\r\n\r\n  1) Root should find all there is\r\n     54 == 42\r\nAssertionError: 54 == 42\r\n    at Context.<anonymous> (.../mocha/test2.js:5:9)\r\n    ... // Abbreviated\r\n```\r\n\r\nThis example shows a failed test, displaying some details and the stack-trace that is abbreviated here in the output for our examples. Let's keep this failing test around for now.\r\n\r\nGreat! However, now, the tests appear to run all in the same top-level test-suite - we cannot easily distinguish which tests were run in which task. Let's change this. Add a ```suite:true``` to each task, turning each task into a virtual test-suite that creates a new level of test-suites in the test-results using the ```title``` given.\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\" },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false }\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\r\n\t\"tasks\": [\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Everything\",\r\n\t\t\t\"suite\": true,\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t}\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Calculations\",\r\n\t\t\t\"suite\": true,\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test2.js\"]\r\n\t\t\t}\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Phrasing\",\r\n\t\t\t\"suite\": true,\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test3.js\"]\r\n\t\t\t}\r\n\t\t}\r\n\t]\r\n};\r\n```\r\nThe output for this configuration is:\r\n```shell\r\n  Everything\r\n    ✓ should know the answer to life, the universe, and everything\r\n\r\n    The End\r\n      ✓ should print something\r\n\r\n  Calculations\r\n    1) should find all there is\r\n    ✓ should calculate the worst first million years for marvin\r\n\r\n  Phrasing\r\n    ✓ should not panic\r\n    ✓ should panic\r\n\r\n  1) Calculations should find all there is\r\n     54 == 42\r\nAssertionError: 54 == 42\r\n    at Context.<anonymous> (.../mocha/test2.js:5:9)\r\n    ... // Abbreviated\r\n```\r\nGreat! That looks already a lot better.\r\n\r\nBut now, I want to run the first tests in parallel, and the last test should run when the first two tests are both completed. Simply wrap the first two tasks in an array literal. This will group the tasks together.\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\" },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false }\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\r\n\t\"tasks\": [\r\n\t\t[{ // <-----\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Everything\",\r\n\t\t\t\"suite\": true,\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t}\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Calculations\",\r\n\t\t\t\"suite\": true,\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test2.js\"]\r\n\t\t\t}\r\n\t\t}], // <-----\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"title\": \"Phrasing\",\r\n\t\t\t\"suite\": true,\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test3.js\"]\r\n\t\t\t}\r\n\t\t}\r\n\t]\r\n};\r\n\r\n```\r\nLet's run it again:\r\n```shell\r\n  Everything\r\n\r\n  Calculations\r\n    ✓ should know the answer to life, the universe, and everything\r\n\r\n    The End\r\n      ✓ should print something\r\n    1) should find all there is\r\n    ✓ should calculate the worst first million years for marvin\r\n\r\n  Phrasing\r\n    ✓ should not panic\r\n    ✓ should panic\r\n\r\n  1) Calculations should find all there is\r\n     54 == 42\r\nAssertionError: 54 == 42\r\n    at Context.<anonymous> (.../mocha/test2.js:5:9)\r\n    ... // Abbreviated\r\n```\r\nYou can see that the tests are running in parallel now. However, the ```Spec``` reporter prints the test-results as soon as it receives them from the test-clients, turning the output into a mess. This is the default behavior of most of the console reporters. We can change this behavior by setting the ```progress``` property to ```false```. When this flag is set to ```false```, the reporter will organize all test-results in a tree, printing them in an ordered manner just when Preceptor completes all tests.\r\n\r\nTo make it a little bit more interesting, let's add also the ```Duration``` reporter; it will print the total duration. The value of ```progress``` for the ```Duration``` reporter is by default ```false```, so no need to set this explicitly.\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\", \"progress\": false },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false },\r\n\t\t\t\t{ \"type\": \"Duration\" }\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\r\n\t// ...\r\n};\r\n```\r\nThe output is now:\r\n```shell\r\n  Everything\r\n    ✓ should know the answer to life, the universe, and everything\r\n\r\n    The End\r\n      ✓ should print something\r\n\r\n  Calculations\r\n    1) should find all there is\r\n    ✓ should calculate the worst first million years for marvin\r\n\r\n  Phrasing\r\n    ✓ should not panic\r\n    ✓ should panic\r\n\r\n  1) Calculations should find all there is\r\n     54 == 42\r\nAssertionError: 54 == 42\r\n    at Context.<anonymous> (.../mocha/test2.js:5:9)\r\n    ... // Abbreviated\r\n\r\nTime: 531 milliseconds\r\n```\r\nThe test-results are printed correctly now. Also, the duration is printed just as we wanted it.\r\n\r\nWith Preceptor, you can add as many reporters as you want, all of which are run at the same time - no matter if they are console or file reporters.\r\n\r\nLet's add some file reporters: JUnit and TAP\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\", \"progress\": false },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false },\r\n\t\t\t\t{ \"type\": \"Duration\" },\r\n\t\t\t\t{ \"type\": \"Junit\", path: \"junit.xml\" },\r\n\t\t\t\t{ \"type\": \"Tap\", path: \"tap.txt\" }\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\r\n\t// ...\r\n};\r\n```\r\nWhen running this configuration, two files will be created in the current working directory called ```junit.xml``` and ```tap.txt```, holding JUnit and TAP results respectively.\r\n\r\nNow, I want to collect code-coverage reports from all the tests that were run. For that, add a ```coverage``` section to the global Preceptor configuration, activating it with ```active:true```, and may be supply additional configuration options.\r\n```javascript\r\nmodule.exports = {\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": {\r\n\t\t\t\"reporter\": [\r\n\t\t\t\t{ \"type\": \"Spec\", \"progress\": false },\r\n\t\t\t\t{ \"type\": \"List\", \"progress\": false },\r\n\t\t\t\t{ \"type\": \"Duration\" },\r\n\t\t\t\t{ \"type\": \"Junit\", path: \"junit.xml\" },\r\n\t\t\t\t{ \"type\": \"Tap\", path: \"tap.txt\" }\r\n\t\t\t]\r\n\t\t},\r\n\t\t\"coverage\": {\r\n\t\t\t\"active\": true, // Activate coverage\r\n\r\n\t\t\t\"path\": \"./coverage\" // This is the default path\r\n\r\n\t\t\t// Add here any other coverage options.\r\n\t\t\t// See below for more information\r\n\t\t}\r\n\t},\r\n\r\n  // ...\r\n};\r\n```\r\nThe output is now:\r\n```shell\r\nEverything\r\n  ✓ should know the answer to life, the universe, and everything\r\n\r\n  The End\r\n    ✓ should print something\r\n\r\nCalculations\r\n  1) should find all there is\r\n  ✓ should calculate the worst first million years for marvin\r\n\r\nPhrasing\r\n  ✓ should not panic\r\n  ✓ should panic\r\n\r\n1) Calculations should find all there is\r\n   54 == 42\r\nAssertionError: 54 == 42\r\n  at Context.<anonymous> (.../mocha/test2.js:5:9)\r\n  ... // Abbreviated\r\n\r\nTime: 572 milliseconds\r\n\r\n\r\n=============================== Coverage summary ===============================\r\nStatements   : 96.3% ( 26/27 )\r\nBranches     : 100% ( 0/0 )\r\nFunctions    : 92.31% ( 12/13 )\r\nLines        : 96.3% ( 26/27 )\r\n================================================================================\r\n```\r\n\r\nBy default, the coverage reporter creates three reports:\r\n* ```text-summary``` - The short coverage summary in the console\r\n* ```file``` - JSON file holding all coverage details, created in the ```path``` directory\r\n* ```lcov``` - HTML report in the ```path``` directory\r\n\r\nThis concludes the introduction of Preceptor. Please read the general overview that follows, or refer to the API documentation that is included with this module.\r\n\r\n## Usage\r\n\r\n###Command-Line usage\r\n\r\nPreceptor is started through the command-line that can be found in the ```bin``` folder. You can run the application with\r\n\r\n```shell\r\npreceptor [options] [config-file]\r\n```\r\n\r\nThe ```config-file``` is by default ```rule-book.js``` or ```rule-book.json``` when left-off, where by the first one has priority over the second.\r\n\r\nThe command-line tool exposes a couple of flags and parameters:\r\n```\r\n--config j          Inline JSON configuration or configuration overwrites\r\n--profile p         Profile of configuration\r\n--subprofile p      Sub-profile of configuration\r\n--version           Print version\r\n--help              This help\r\n```\r\n\r\nFor profiles, see the profile section below.\r\n\r\nThe ```config``` options adds the possibility to create or overwrite configuration directly from the console. The inline configuration-options are applied after the profile selection. \r\nObjects are merged if a configuration file was selected, and arrays will be appended to already available lists.\r\n\r\n###Testing Lifecycle\r\nTo understand how Preceptor works, we have to understand first what the testing lifecycle is. Tests are run usually through a common set of states that can be defined as the testing lifecycle.\r\n\r\nThis include:\r\n* start of all tests (not all frameworks support this)\r\n* start of test suites\r\n* start of tests\r\n* end of tests (with different types of results)\r\n* end of test suites\r\n* end of all tests  (not all frameworks support this)\r\n\r\nEach individual state will be triggered in Preceptor so that plugins can react on these state changes. This includes the reporter, the client, the client decorators, the tasks, and Preceptor itself.\r\n\r\nPreceptor breaks them (and some additional states) into 4 groups:\r\n* Admin - Administrative states\r\n* Suite - Suite management\r\n* Test - Test management\r\n* Item - Item management for parts of a test\r\n\r\nLet's describe them in more detail by describing their function and what information in conveyed.\r\n\r\n\r\n####Administrative states\r\nAdministrative states will be triggered for events that change the collection and run behavior of Preceptor itself.\r\n\r\n\r\n#####State: Start\r\nThe ```start``` state will be triggered just when Preceptor starts to run.\r\n\r\nParameters:\r\n* __none__\r\n\r\n\r\n#####State: Stop\r\nThe ```stop``` state will be triggered just when Preceptor completes all tests.\r\n\r\nParameters:\r\n* __none__\r\n\r\n\r\n#####State: Complete\r\nThe ```complete``` state will be triggered when Preceptor completed all the export jobs.\r\n\r\nParameters:\r\n* __none__\r\n\r\n\r\n####Suite management\r\nThese states handle any changes to test-suites.\r\n\r\n\r\n#####State: Suite Start\r\nThe ```suiteStart``` state will be triggered when a new test-suite starts. Please be aware that it is possible to have test-suites within other test-suites.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test-suite. Preceptor creates a unique identifier for each state to be able to identifier related state changes.\r\n* ```parentId``` - Parent identifier of parent test-suite. This makes it possible for Preceptor to create a tree of tests and test-suites. For the first test-suite, this will be the id of the ```start``` event.\r\n* ```suiteName``` - Name of the suite that should be used for reporting.\r\n\r\n\r\n#####State: Suite End\r\nThe ```suiteEnd``` state will be triggered when a test-suite completed.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test-suite, the same identifier that was given to ```suiteStart```.\r\n\r\n\r\n\r\n####Test management\r\nTest management states are states that transmit test and test-result information.\r\n\r\n#####State: Test Start\r\nThe ```testStart``` state will be triggered when a new test starts.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test. Preceptor creates a unique identifier for each state to be able to identifier related state changes.\r\n* ```parentId``` - Parent identifier of parent test-suite. This makes it possible for Preceptor to create a tree of tests and test-suites. For the first test without a parent test-suite, this will be the id of the ```start``` event.\r\n* ```testName``` - Name of the test that should be used for reporting.\r\n\r\n\r\n#####State: Test Passed\r\nThe ```testPassed``` state will be triggered when a test has passed.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test that was given to the related ```testStart```.\r\n\r\n\r\n#####State: Test Failed\r\nThe ```testFailed``` state will be triggered when a test has failed. A failure is an unexpected test-result, often triggered by assertions.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test that was given to the related ```testStart```.\r\n* ```message``` - Short description of the failure\r\n* ```reason``` - Detailed description of the failure (often with stack-trace)\r\n\r\n\r\n#####State: Test Error\r\nThe ```testError``` state will be triggered when a test had an error. An error is an unexpected exception that was not triggered by an assertion.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test that was given to the related ```testStart```.\r\n* ```message``` - Short description of the error\r\n* ```reason``` - Detailed description of the error (often with stack-trace)\r\n\r\n\r\n#####State: Test Undefined\r\nThe ```testUndefined``` state will be triggered when a test hasn't been defined. This is triggered when the testing-framework recognizes a test, but could not find the test-implementation. Not all testing frameworks support this state.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test that was given to the related ```testStart```.\r\n\r\n\r\n#####State: Test Skipped\r\nThe ```testSkipped``` state will be triggered when a test has been skipped. Tests are usually skipped when for example sub-systems that are dependencies for test are not available.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test that was given to the related ```testStart```.\r\n* ```reason``` - Reason for skipping this test.\r\n\r\n\r\n#####State: Test Incomplete\r\nThe ```testIncomplete``` state will be triggered when a test is available, but incomplete. This happens when tests are written, but are not completed yet.\r\n\r\nParameters:\r\n* ```id``` - Identifier of test that was given to the related ```testStart```.\r\n\r\n\r\n###Configuration\r\n\r\nThe configuration file has three top-level properties:\r\n* ```configuration``` - Global Preceptor configuration that describes Preceptor behavior.\r\n* ```shared``` - Shared task options.\r\n* ```tasks``` - Individual task options which are run by default in sequence.\r\n\r\n####Profiles\r\nThe configuration file supports multiple layers of profiles:\r\n * Global Profile\r\n * Tasks Profile\r\n\r\nAs an example, let's use the following abbreviated configuration:\r\n\r\n__config.js__\r\n```javascript\r\nmodule.exports = {\r\n\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": { \"reporter\": [ { \"type\": \"Spec\" } ] }\r\n\t},\r\n\r\n\t\"tasks\": [\r\n\t\t{\r\n\t\t\t\"type\": \"mocha\",\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t}\r\n\t\t}\r\n\t]\r\n};\r\n```\r\n\r\n#####Global Profile\r\nGlobal profiles can be used to toggle between full-configurations:\r\n\r\n```javascript\r\nmodule.exports = {\r\n\r\n\t\"profile1\": {\r\n\t\t\"configuration\": {\r\n\t\t\t\"reportManager\": { \"reporter\": [ { \"type\": \"Spec\" } ] }\r\n\t\t},\r\n\t\r\n\t\t\"tasks\": [\r\n\t\t\t{\r\n\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t]\r\n\t},\r\n\r\n\t\"profile2\": {\r\n\t\t\"configuration\": {\r\n\t\t\t\"reportManager\": { \"reporter\": [ { \"type\": \"Dot\" } ] }\r\n\t\t},\r\n\t\r\n\t\t\"tasks\": [\r\n\t\t\t{\r\n\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\"paths\": [__dirname + \"/mocha/test2.js\"]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n};\r\n```\r\n\r\nTo select the second profile, call Preceptor as follows:\r\n```shell\r\npreceptor --profile profile2 config.js\r\n```\r\n\r\nThe first profile can be selected by supplying ```profile1``` instead, or whatever name you give the corresponding profiles.\r\nBe sure to always supply a profile when it is available since the default behavior is to not look for a profile.\r\n\r\n#####Tasks Profile\r\nTask profiles are very similar to global profiles, but are there for toggling task-lists. This can be very useful, when global configurations are shared between multiple profiles. Task profiles are also called sub-profiles since they toggle configuration below the global profile selection.\r\n\r\nAn example looks like this:\r\n```javascript\r\nmodule.exports = {\r\n\r\n\t\"configuration\": {\r\n\t\t\"reportManager\": { \"reporter\": [ { \"type\": \"Spec\" } ] }\r\n\t},\r\n\r\n\t\"tasks\": {\r\n\t \t\"sub-profile1\": [\r\n\t\t\t{\r\n\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\"paths\": [__dirname + \"/mocha/test1.js\"]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t],\r\n\t\t\"sub-profile2\": [\r\n\t\t\t{\r\n\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\"paths\": [__dirname + \"/mocha/test2.js\"]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n};\r\n```\r\n\r\nTo select the first sub-profile, call Preceptor as follows:\r\n```shell\r\npreceptor --subprofile sub-profile1 config.js\r\n```\r\n\r\nBe sure to always supply a sub-profile when it is available since the default behavior is to not look for a sub-profile.\r\n\r\n#####Combine Profiles\r\nIt is also possible to combine both profile methods.\r\n\r\n```javascript\r\nmodule.exports = {\r\n\r\n\t\"ci\": {\r\n\t\t\"configuration\": {\r\n\t\t\t\"reportManager\": { \"reporter\": [ { \"type\": \"Dot\" } ] }\r\n\t\t},\r\n\t\r\n\t\t\"tasks\": {\r\n\t\t\t\"acceptance\": [\r\n\t\t\t\t{\r\n\t\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\t\"paths\": [__dirname + \"/build/mocha/test1.js\"]\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t],\r\n\t\t\t\"integration\": [\r\n\t\t\t\t{\r\n\t\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\t\"paths\": [__dirname + \"/build/mocha/test2.js\"]\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t},\r\n\t\"dev\": {\r\n\t\t\"configuration\": {\r\n\t\t\t\"reportManager\": { \"reporter\": [ { \"type\": \"Spec\" } ] }\r\n\t\t},\r\n\t\r\n\t\t\"tasks\": {\r\n\t\t\t\"acceptance\": [\r\n\t\t\t\t{\r\n\t\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\t\"paths\": [__dirname + \"/../mocha/test1.js\"]\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t],\r\n\t\t\t\"integration\": [\r\n\t\t\t\t{\r\n\t\t\t\t\t\"type\": \"mocha\",\r\n\t\t\t\t\t\"configuration\": {\r\n\t\t\t\t\t\t\"paths\": [__dirname + \"/../mocha/test2.js\"]\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n};\r\n```\r\n\r\nWith this example, you could run the acceptance tests from your local machine with:\r\n```shell\r\npreceptor --profile dev --subprofile acceptance config.js\r\n```\r\n\r\n####Global configuration\r\nPreceptors task-independent behavior is configured in this section. It has the following properties:\r\n\r\n* ```verbose``` - Flag for verbose output (default: false)\r\n* ```reportManager``` - Reporting configuration\r\n* ```reportManager.reporter``` - List of loaded reporters\r\n* ```reportManager.listener``` - List of loaded listeners\r\n* ```coverage``` - Coverage option\r\n* ```plugins``` - List of plugin modules to load\r\n\r\n######Reporting\r\nThe report manager describes what reporters should be used and what it should listen for to receive testing lifecycle events from unsupported clients.\r\nSee the ```preceptor-reporter``` project documentation for more information on the ```reportManager``` property.\r\n\r\n######Coverage\r\nCoverage report collection can be configured in this section. It has the following options:\r\n* ```active``` - Flag that turns coverage collection on or off (default: false - off)\r\n* ```root``` - Root of coverage data. This is used to reduce depth of coverage paths.\r\n* ```path``` - Path to where the file coverage reports should be exported to.\r\n* ```reports``` - List of reports to export to. (default: ['file', 'lcov', 'text-summary'])\r\n* ```includes``` - List of patterns for files to include scripts into the coverage report. (default: ['**/*.js'])\r\n* ```excludes``` - List of patterns to exclude files and directories of scripts that were previously whitelisted with the ```includes``` option. (default: ['**/node_modules/**', '**/test/**', '**/tests/**'])\r\n\r\nThis feature uses Istanbul for collecting and merging coverage reports. See the Istanbul project [website](https://github.com/gotwarlost/istanbul) for more information.\r\n\r\n__Additional Reports__\r\nPreceptor adds the ```file``` report to the list of reports to be able to disable the export of the JSON data. The ```file``` value will not be given to Istanbul since it is not available there.\r\n\r\n####Shared configuration\r\nAny value that is assigned to the 'shared' object in the configuration root will be assigned as default for all task options. Task options then can overwrite these values. This gives a developer the opportunity to set own default values for properties, overwriting the default values that were given by the system.\r\n\r\n######Plugins\r\nThe Preceptor system supports the loading of custom plugins by installing the modules through NPM, and by adding the module name to this list. Preceptor then tries to load each of these modules by executing the ```loader``` function on the exported module interface, giving it the instance of Preceptor itself. The plugin can then register itself to Preceptor. See the ```preceptor-webdriver``` project for an example.\r\n\r\n###Tasks\r\n\r\nThe following tasks for testing-frameworks are supported by default:\r\n* Cucumber - Cucumber.js testing framework\r\n* Mocha - Mocha unit testing framework\r\n* Kobold - Kobold visual regression testing framework\r\n\r\nPreceptor implements also some general purpose tasks:\r\n* Node - Running node scripts\r\n* Shell - Running shell scripts\r\n* Group - Grouping tasks\r\n\r\nThe following sections describe the configuration and usage of these tasks\r\n\r\n####General Configuration\r\nTasks have a common set of configuration options that can be set on the root of the task. Any custom configuration option, that will be listed for each task below, should be set on the ```configuration``` object instead of the root (see \"Object and List Configuration\"). The reason for this separation is flexibility for new features in future version of Preceptor without breaking any task or custom task that might add similar  named options.\r\n\r\n#####Value Configuration\r\n* ```taskId``` - Identifier of task. If none is given, then Preceptor will assign an identifier and may use it in error reports.\r\n* ```type``` - Identifier for task-plugin. For example, ```type:'mocha'``` will use the Mocha task-plugin.\r\n* ```name``` - Name of task. This is a more user friendly version of ```taskId```. If none is given, then it uses the ```taskId``` value.\r\n* ```title``` - Description of task that might be used in test-results.\r\n\r\n#####Object and List Configuration\r\n* ```configuration``` - Custom configuration for task-specific options. See below for the task options.\r\n* ```decorators``` - List of client decorators that should be loaded on the client. See \"Client Decorators\" section for more information.\r\n\r\n#####Flag Configuration\r\n* ```active``` - Flag that defines if task is active or inactive. Inactive tasks will be ignored by Preceptor, skipping any tests and results from the task. (default: true)\r\n* ```suite``` - Flag that defines if task should be used as a virtual test-suite. A virtual test-suite injects itself into the test-results. This makes it possible to compose the test-results however it is needed. (default: false)\r\n* ```debug``` - Flag that defines if task is run in debug mode. In debug-mode, the client is run directly in the Preceptor process. Output is not caught by Preceptor and directly printed to std-out, and a breakpoint will stop the Preceptor process. Avoid using this flag long-term. (default: false)\r\n* ```report``` - Flag that determines if task uses the globally configured reporter. (default: true) If deactivated, testing life-cycle events will be muted. \r\n* ```coverage``` - Flag that determines if task uses the globally configured coverage collector. (default: false) If deactivated, collected coverage won't be merged \r\n* ```verbose``` - Prints every step taken by Preceptor. This adds a lot of output and might be overwhelming at first. (default: false)\r\n* ```failOnError``` - Flag that defines if task should skip all other tests and fail Preceptor. (default: false)\r\n* ```echoStdOut``` - Flag that defines if task should echo all std-out data.\r\n* ```echoStdErr``` - Flag that defines if task should echo all std-err data.\r\n\r\n\r\n####Cucumber Task\r\n\r\nThe ```type```-value for this task is ```cucumber```.\r\n\r\nTask options:\r\n* ```path``` - Path to the tests (required)\r\n* ```tags``` - List of tags to accept. These can be strings with one tag, or an array of tags. These tags should have an '@' and may be the '~' if required. See Cucumber.js documentation for more information.\r\n* ```format``` - Output format for test results (default: 'progress')\r\n* ```functions``` - A list of functions to import (still WIP). The function can have a ```name``` property to be used in verbose logging.\r\n* ```coffeScript``` - Flag that defines that output should be in coffee-script.\r\n\r\n\r\n####Mocha Task\r\n\r\nThe ```type```-value for this task is ```mocha```.\r\n\r\nTask options:\r\n* ```reporters``` - Reporter to use (default: 'spec')\r\n* ```ui``` - API interface to use (default: 'bdd')\r\n* ```colors``` - Use colors in output (default: true)\r\n* ```inlineDiff``` - Inline-diff support (default: false)\r\n* ```slow``` - Defines how many milliseconds is considered as a slow test (default: 75)\r\n* ```timeOuts``` - Specifies if time-outs should be enforced (default: true)\r\n* ```timeOut``` - Describes when a test is considered to be in time-out (in milliseconds) (default: 2000)\r\n* ```bail``` - Stop execution on the first error (default: false)\r\n* ```grep``` - Filter to use for tests (default: false - off)\r\n* ```invert``` - Invert ```grep``` filter (default: false)\r\n* ```checkLeaks``` - Checks for memory leaks (default: false)\r\n* ```asyncOnly``` - Enforce all tests to be asynchronous (default: false)\r\n* ```globals``` - List of defined globals\r\n* ```paths``` - A list of paths to the tests (default: ['test'])\r\n* ```functions``` - A list of functions to import as tests. The function can have a ```name``` property to be used in verbose logging.\r\n* ```recursive``` - Recursive search in the test paths (default: false)\r\n* ```require``` - List of files to be require'd before tests run.\r\n* ```sort``` - Sort tests before running the tests (default: false)\r\n\r\n\r\n####Kobold Task\r\n\r\nThe ```type```-value for this task is ```kobold```.\r\n\r\nTask options:\r\n* ```verbose``` - Verbose output (default: true)\r\n* ```failForOrphans``` - Flag that defines if Kobold should fail tests when Orphans are encountered. Orphans are screens that were previously approved, but are missing in the most recent test-run. This can be the case when tests are removed, or when the testing-framework was interrupted. (default: true)\r\n* ```failOnAdditions``` - Flag that defines if Kobold should fail tests when Additions are encountered. Additions are screens that have never been seen (approved) before, but that are available during the most recent test-run. These usually happen when tests are added; these screens should be reviewed before approving. (default: true)\r\n* ```build``` - Identifier for the current build. This can be an identifier from a CI or a random generated one. This value is used to distinguish multiple test-runs from each-other, and is given to the remote storage. (default: process.env.BUILD_NUMBER or process.env.USER + <timestamp>)\r\n* ```blinkDiff``` - Configuration for the blink-diff tool.\r\n* ```mocha``` - Mocha options for Kobold tests since it is build on-top of Mocha. (See above)\r\n* ```storage``` - Storage configuration for Kobold. See the project website for more information.\r\n* ```source``` - Source of screens. Uses ```storage``` as default. See the project website for more information.\r\n* ```destination``` - Destination for screens. Uses ```storage``` as default. See the project website for more information.\r\n\r\nThe storage configuration has also the following values by default:\r\n* ```type``` - 'File'\r\n* ```options.approvedFolderName``` - 'approved'\r\n* ```options.buildFolderName``` - 'build'\r\n* ```options.highlightFolderName``` - 'highlight'\r\n\r\n\r\n####Loader Task\r\n\r\nThis task imports already available test-report files, including JUnit and TAP files. The ```type```-value for this task is ```loader```.\r\n\r\nTask options:\r\n* ```format``` - Format of file-import (i.e. ```junit```, ```tap``` or ```istanbul```). See the [Preceptor-Reporter](http://yahoo.github.io/preceptor-reporter/#loader) project for all available options.\r\n* ```path``` - Glob to select files that should be imported\r\n* ```configuration``` - Custom configuration for each file-format type\r\n\r\n__Example:__\r\n```javascript\r\n\t{\r\n\t\t\"type\": \"loader\",\r\n\t\t\"title\": \"JUnit import\",\r\n\t\t\"suite\": true, // Wrap it in a suite using the title from above\r\n\t\t\r\n\t\t\"configuration\": { // Loader-task specific configuration\r\n\t\t\r\n\t\t\t\"format\": \"junit\", // Use \"junit\" as the format (is default)\r\n\t\t\t\"path\": \"junit-*.xml\", // Glob to select import files\r\n\t\t\t\r\n\t\t\t\"configuration\": {\r\n\t\t\t\t// Custom JUnit loader configuration\r\n\t\t\t\t\"topLevel\": false\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n```\r\n\r\n####Node Task\r\n\r\nThe ```type```-value for this task is ```node```.\r\n\r\nTask options:\r\n* ```path``` - Path to JavaScript file that should be executed. The JavaScript file will be running in its own process.\r\n\r\n\r\n####Shell Task\r\n\r\nThe ```type```-value for this task is ```shell```.\r\n\r\nTask options:\r\n* ```cwd``` - Defines the current working directory for the script.\r\n* ```env``` - List of key-value pairs (in an object) of environment variables set for the shell script. (default: {})\r\n* ```cmd``` - Command to be executed in shell. The command will be run in its own process.\r\n\r\n\r\n####Group Task\r\n\r\nThe ```type```-value for this task is ```group```.\r\n\r\nTask options:\r\n* ```parallel``` - Flag that defines if tasks should be run in parallel (default: false)\r\n* ```tasks``` - List of task-options that should be run within this group.\r\n\r\nThe ```group``` Task Decorator uses this task to add the array-literal feature to the configuration.\r\n\r\n\r\n###Task Decorators\r\n\r\nA task decorator is a plugin that can modify task-options before they are applied. This makes the configuration very flexible as the whole configuration scheme can be changed with these plugins.\r\n\r\nThe following task decorators are build-in:\r\n* ```group``` - Adds the array-literal feature for running tasks in parallel. It replaces the array-literal with a group that has the ```parallel``` flag set, injecting all the tasks into the ```task``` options of the group-task.\r\n* ```identifier``` - Makes sure that identifiers are given for ```taskId```, ```name```, and ```title```. ```taskId``` will receive an unique id if not given, and all others might inherit the value if they are also missing - ```name``` from ```taskId``` and ```title``` from ```name```.\r\n\r\n\r\n###Client Decorators\r\n\r\nClient decorators are \"tasks\" that will be run in the client process by the client-runner. The decorators attach themselves to test-hooks that are triggered during the testing lifecycle (see above).\r\n\r\nThere are two types of hooks:\r\n* Event hooks - Events that trigger without the possibility to halt the testing framework. Event hooks are triggered for all events during the testing lifecycle (see above).\r\n* Activity hooks - Methods that are triggered that should return a Promise. This gives the client decorator the possibility to halt the testing framework for a while to execute some other tasks first.\r\n\r\nThere are four activity hooks:\r\n* ```processBefore``` - Triggered once before a suite of tests.\r\n* ```processBeforeEach``` - Triggered before each test-case.\r\n* ```processAfterEach``` - Triggered after each test-case.\r\n* ```processAfter``` - Triggered once after a suite of tests.\r\n\r\nOut of the box, Preceptor supports the following client decorators:\r\n* ```plain``` - Prints each of the triggers to the console. This client-decorator should be used for debugging purposes when creating client-decorators.\r\n\r\nThe ```preceptor-webdriver``` plugin adds the WebDriver client-decorator for injecting Selenium setup/tear-down code into the client.\r\n\r\nClient decorators can be added to every task that supports the testing lifecycle events (generally all testing frameworks), and it is added through the ```decorators``` list in the task-options.\r\n\r\n####Configuration\r\nClient decorators has the following common options:\r\n* ```type``` - Name of the plugin for client decorators\r\n* ```configuration``` - Client decorator specific configuration\r\n\r\nThe general client decoration configuration namespace is reserved for future changes to all client decorators. Client decorator specific configuration should be described in ```configuration```.\r\n\r\n\r\n###Client Runner\r\nClients are generally run in its own process. Each of these processes communicates with Preceptor, making it also possible to instruct the client to run client decorators in its process. For this to work, each testing-framework needs a client-runner. This is abstracted away, but is required when writing a plugin. The following client-runners are available:\r\n* ```cucumber``` - Runs Cucumber.js\r\n* ```mocha``` - Runs Mocha\r\n* ```kobold``` - Runs Kobold. Uses internally the ```mocha``` client-runner.\r\n* ```node``` - Runs Node.JS scripts\r\n\r\nPlease see the above files in the source-code for details on how to implement these. Much of the common behavior and the communication is implemented in the ```client.js``` file.\r\n\r\n\r\n###Plugin Naming\r\nThe plugin module naming should follow the Grunt plugin naming convention. Plugins contributed by the community should have \"contrib\" in the module name (example: ```preceptor-contrib-hello-world```). Plugins that are supported by the Preceptor team will be named without the \"contrib\" keyword (example: ```preceptor-webdriver```).\r\n\r\n\r\n##API-Documentation\r\n\r\nGenerate the documentation with following command:\r\n```shell\r\nnpm run docs\r\n```\r\nThe documentation will be generated in the ```docs``` folder of the module root.\r\n\r\n##Tests\r\n\r\nRun the tests with the following command:\r\n```shell\r\nnpm run test\r\n```\r\nThe code-coverage will be written to the ```coverage``` folder in the module root.\r\n\r\n##Third-party libraries\r\n\r\nThe following third-party libraries are used by this module:\r\n\r\n###Dependencies\r\n* glob: https://github.com/isaacs/node-glob\r\n* istanbul: https://github.com/gotwarlost/istanbul\r\n* log4js: https://github.com/nomiddlename/log4js-node\r\n* minimatch: https://github.com/isaacs/minimatch\r\n* mkdirp: https://github.com/substack/node-mkdirp\r\n* preceptor-core: https://github.com/yahoo/preceptor-core\r\n* preceptor-reporter: https://github.com/yahoo/preceptor-reporter\r\n* promise: https://github.com/then/promise\r\n* underscore: http://underscorejs.org\r\n* uuid: https://github.com/shtylman/node-uuid\r\n\r\n###Dev-Dependencies\r\n* chai: http://chaijs.com\r\n* cucumber: http://github.com/cucumber/cucumber-js\r\n* mocha: https://github.com/visionmedia/mocha\r\n* cabbie: https://github.com/ForbesLindesay/cabbie\r\n* kobold: https://github.com/yahoo/kobold\r\n* preceptor-webdriver: https://github.com/yahoo/preceptor-webdriver\r\n* yuidocjs: https://github.com/yui/yuidoc\r\n\r\n##License\r\n\r\nThe MIT License\r\n\r\nCopyright 2014 Yahoo Inc.\r\n","google":"UA-56408730-4","note":"Don't delete this file! It's used internally to help with page regeneration."}